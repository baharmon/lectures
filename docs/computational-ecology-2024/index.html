<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
<title>Computational Ecology for Landscape Architects</title>


<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="/lectures/reveal-js/css/reset.css">
<link rel="stylesheet" href="/lectures/reveal-js/css/reveal.css"><link rel="stylesheet" href="/lectures/reveal-hugo/themes/static-custom-theme.css" id="theme"><link rel="stylesheet" href="/lectures/highlight-js/default.min.css">
    
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
  

    <section><h1 id="computational-ecology">Computational Ecology</h1>
<h3 id="for-landscape-architects">for Landscape Architects</h3>
<p><a href="https://baharmon.github.io/">Brendan Harmon</a></p>
<img height="50px" src="lsu-coad-logo.png">



<aside class="notes">
Hello. 
I am Brendan Harmon,
an assistant professor of landscape architecture
at Louisiana State University.
Today I will be talking about
my recent research and teaching.
While I have expertise in the spatial sciences,
recently I have been focused on 
computational ecology and 
computational design 
- on computational methods 
for ecological research
and the creative use of computation 
in the design process.
This talk will explore several applications
of computational ecology and design -
the preservation of heritage landscapes,
planting and remote sensing with robots,
lidar analytics and biomass estimation, 
and point cloud modeling. 
</aside>
</section><section>
<h1 id="robotics">Robotics</h1>



<aside class="notes">
I am exploring creative applications for robots.
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="ecological-robotics-7.jpg">
  
<h1 id="ecological-robotics">Ecological Robotics</h1>



<aside class="notes">
I have been developing methods
for robotic planting 
in the lab and the field.
Here, for example, 
is the field robot that I will use 
for autonomous planting in the field.
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="ecological-robotics-2.jpg"
      data-background-size="contain">
  



<aside class="notes">
In the lab I developed a process 
for 3D printing with seeds.
I use a robotic system to extrude seeds 
in a paste of clay, planting media, and water.
With robotic paste-based extrusion,
seeds can be precisely planted
in computationally generated patterns.
Initially I tested this process in the lab,
printing in small trays
with mixes of seeds
in different patterns.
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="ecological-robotics-3.jpg"
      data-background-size="contain">
  



<aside class="notes">
This prototype uses Grasshopper 
to generate planting designs
and program the robot and extruder.
With this process, 
planting patterns could be generated from
procedural noise gradients,
cellular automata,
space filling curves,
AI image generation,
and other algorithms.
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="ecological-robotics-4.jpg"
      data-background-size="contain">
  



<aside class="notes">
To scale up, I will deploy this
planting system on a field robot.
I plan to experiment with other 
methods for autonomous planting
such as seed hoppers.
Eventually I plan to conduct 
a controlled field experiment.
After autonomously seeding test plots,
I will use lidar to monitor growth.
With autonomous seeding at field scale,
landscapes can be 
planted iteratively and adaptively
and designed for ecological performance
with new computational aesthetics. 

</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="ecological-robotics-5.jpg">
  



<aside class="notes">
With robotic planting,
ecological gradients - for example -
can be computationally designed and
and autonomously planted.
Here is a gradient of procedural noise.
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="ecological-robotics-6.jpg">
  



<aside class="notes">
And here are seedlings
planted in a prodecural noise gradient. 
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="echo-1.jpg">
  
<h1 id="echo">Echo</h1>



<aside class="notes">

With new media artist Hye Yeon Nam
sound artist Ka Hei Cheng, 
I used autonomous planting to highlight
the entanglement of nature and technology.
We transformed a 3D printed planting design
into a new musical instrument. 
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-iframe="https://player.vimeo.com/video/801436544"
      data-background-size="cover">
  



<aside class="notes">
By embedding capacitive touch sensors
in the soil,
we transformed plants
into a living interface for
sonic performance. 
Touching the seedlings plays a sample
from a sound palette 
of environmental recordings.
</aside>
</section><section>
<h1 id="computational-ecology-1">Computational Ecology</h1>



<aside class="notes">
My recent research in computational ecology
explores the use of lidar
to estimate biomass and carbon. 
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="hilltop-1.jpg">
  
<h1 id="drone-data-analytics">Drone Data Analytics</h1>



<aside class="notes">
Since 2020 I have been using drones
with lidar and multispectral sensors
to study the evolution of the meadow
established at LSU's Hilltop Arboretum.
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="hilltop-3.jpg"
      data-background-size="contain">
  



<aside class="notes">
With regular aerial surveys 
I can map fluxes of aboveground biomass and carbon
in the meadow. 
Here, for example, is 
a 3D scatterplot of
the net annual biomass of the meadow
in its first year. 
By accounting for carbon storage in meadows and prairies,
we can demonstrate their ecoystem services
and advocate for their creation or conservation.
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="duelling-oak.jpg"
      data-background-size="contain">
  
<h1 id="atlas-of-heritage-trees">Atlas of Heritage Trees</h1>



<aside class="notes">
For another project 
- the Atlas of Heritage Trees -
I am laser scanning ancient trees
of significant historical, 
cultural, and ecological importance.
Louisiana has many large, old, 
and culturally significant specimens
of southern live oak and bald cypress. 
These trees are charismatic megaflora â€“ 
specimens that capture 
the imagination of the public 
and encourage broader support 
for biodiversity conservation.
To preserve a record of these 
irreplaceable cultural icons, 
we are compiling an Atlas of Heritage Trees.
As a digital humanities project,
this research aims to document and share
the legacy of these heritage trees.
As a work of computational ecology,
this research aims to estimate the 
biomass and carbon of large, old trees
which act as keystone ecological structures. 
</aside>
</section><section>
<img src="big-cypress-point-cloud.jpg" width="500">
<img src="big-cypress-voxels.jpg" width="400">
<img src="big-cypress-marching-cubes.jpg" width="400">
<img src="big-cypress-dendro.jpg" width="400">



<aside class="notes">
As part of this project,
I have developed a method 
for building volumetric models
from laser scanned point clouds. 
This volumetric modeling process 
can be used to calculate 
the volume of large, old trees
with extensive cavities
for biomass and carbon estimation. 
It can also be used to 3D print models 
of these specimens
for outreach, education, and exhibition.
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="big-cypress-print-xl.jpg">
  



<aside class="notes">
Here, for example, 
is a 3D print of the Big Cypress,
a 1500 year old Bald Cypress 
on Cat Island in Louisiana.
It is the largest recorded bald cypress
and the reigning national champion.
</aside>
</section><section>
<h1 id="heritage-preservation">Heritage Preservation</h1>



<aside class="notes">
My research in heritage preservation 
uses remote sensing technologies 
such as lidar
to preserve a record
of disappearing heritage landscapes.
</aside>
</section><section>
<img src="faro-focus.jpg" width="250">
<img src="matrice.jpg" width="600">



<aside class="notes">
I use drones with lidar, 
terrestrial laser scanning,
and neural radiance fields
to record the spatial structure
and phenomenological character
of heritage landscapes.
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="rosedown-landscape.jpg">
  
<h1 id="rosedown">Rosedown</h1>



<aside class="notes">
With funding from the National Park Service,
my colleague Nick Serrano and I
scanned Rosedown Plantation
in St. Francisville, Louisiana.
Rosedown is unique for its 
extant, largely intact plantation gardens;
these gardens are important
not only for being representative of 
plantation garden design in the American South,
but also as artifacts of enslaved labor.
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="rosedown-rockery.jpg">
  



<aside class="notes">
With terrestrial laser scanning,
we captured temporal aspects of the landscape
such as the flowers in bloom on the rockery
- pictured here -
and the spanish moss swaying
on the live oak allee. 
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="rosedown-tunnel.jpg">
  



<aside class="notes">
The scan of the tunnel through the rockery,
captured details such as the pebble wash
and the moss and lichen on the bricks.
This level of immersive detail records
some of sensory experience
and phenomenological character
of the site in ways that other media cannot.
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="rosedown-staircase.jpg">
  



<aside class="notes">
The scan of the service staircase
captures how the tall steps are worn underfoot,
recording an index of the labor of the enslaved.
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="alford-1.jpg">
  
<h1 id="african-american-burial-grounds">African American Burial Grounds</h1>



<aside class="notes">
With an interdisciplinary team of colleagues,
I have begun a long term project
to preserve a record of the burial grounds
of enslaved African Americans 
and their descendants.
In the American South, 
These sites have long faced 
precarious conditions; 
originally built peripheral 
to antebellum plantations, 
today many occupy remnant parcels 
of isolated land. 
Climate change, 
industrial expansion, 
precarious land-tenure records, 
and dwindling populations 
of descendant communities 
threaten these cultural landscapes. 
The aim of this project 
is to develop a methodology for documenting 
the history, material culture, 
ecological character, and soundscapes 
of these neglected heritage sites. 
Here is a point cloud of Alford Cemetery
captured by a drone with a lidar module.
With drone lidar,
we can record these landscapes 
and their surroundings
at centimeter resolution. 
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="alford-2.jpg"
      data-background-size="contain">
  



<aside class="notes">
With terrestrial laser scanning,
we can record gravesites in immersive detail.
We are also experimenting with 
new scanning techniques such as 
neural radiance fields. 
</aside>
</section><section>
<h1 id="point-clouds">Point Clouds</h1>



<aside class="notes">
I am also interested in point clouds
as a new design medium for landscape architecture,
that is both hyper-detailed, yet also abstract.
</aside>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="gaussian-planting-1.jpg">
  
<h1 id="point-cloud-modeling">Point Cloud Modeling</h1>



<aside class="notes">
I have been experimenting with
point cloud modeling techniques.
Complex, detailed scenes can be composited
by segmenting, transforming, and merging point clouds. 
Features can be classified and segmented
either manually or automatically
with algorithms and machine learning techniques.
Segmented point clouds can be transformed manually 
or algorithmically with, for example, Grasshopper.
Here is a simple example of 
randomly scattered laser scanned plants.
</aside>
</section><section>
<h1 id="future-research">Future Research</h1>
<ul>
<li>
<p><strong>Project:</strong> field robotics</p>
</li>
<li>
<p><strong>Paper</strong> computational aesthetics</p>
</li>
<li>
<p><strong>Book:</strong> computational ecology</p>
</li>
<li>
<p><strong>GIS plugins:</strong> earthworks and mass flows of water and sediment</p>
</li>
<li>
<p><strong>Grasshopper plugins:</strong> lidar &amp; geospatial analytics</p>
</li>
</ul>



<aside class="notes">

</aside>
</section><section>
<p>Learn more at
<a href="https://baharmon.github.io/"><strong>baharmon.github.io</strong></a></p>
</section>

  


</div>
      

    </div>
<script type="text/javascript" src=/lectures/reveal-hugo/object-assign.js></script>

<a href="/lectures/reveal-js/css/print/" id="print-location" style="display: none;"></a>
<script type="text/javascript">
  var printLocationElement = document.getElementById('print-location');
  var link = document.createElement('link');
  link.rel = 'stylesheet';
  link.type = 'text/css';
  link.href = printLocationElement.href + (window.location.search.match(/print-pdf/gi) ? 'pdf.css' : 'paper.css');
  document.getElementsByTagName('head')[0].appendChild(link);
</script>

<script type="application/json" id="reveal-hugo-site-params">{"custom_theme":"reveal-hugo/themes/static-custom-theme.css"}</script>
<script type="application/json" id="reveal-hugo-page-params">null</script>

<script src="/lectures/reveal-js/js/reveal.js"></script>

<script type="text/javascript">
  
  
  function camelize(map) {
    if (map) {
      Object.keys(map).forEach(function(k) {
        newK = k.replace(/(\_\w)/g, function(m) { return m[1].toUpperCase() });
        if (newK != k) {
          map[newK] = map[k];
          delete map[k];
        }
      });
    }
    return map;
  }
  
  var revealHugoDefaults = { center: true, controls: true, history: true, progress: true, transition: "slide" };
  var revealHugoSiteParams = JSON.parse(document.getElementById('reveal-hugo-site-params').innerHTML);
  var revealHugoPageParams = JSON.parse(document.getElementById('reveal-hugo-page-params').innerHTML);
  
  var options = Object.assign({},
    camelize(revealHugoDefaults),
    camelize(revealHugoSiteParams),
    camelize(revealHugoPageParams));
  Reveal.initialize(options);
</script>


  
  
  <script type="text/javascript" src="/lectures/reveal-js/plugin/markdown/marked.js"></script>
  
  <script type="text/javascript" src="/lectures/reveal-js/plugin/markdown/markdown.js"></script>
  
  <script type="text/javascript" src="/lectures/reveal-js/plugin/highlight/highlight.js"></script>
  
  <script type="text/javascript" src="/lectures/reveal-js/plugin/zoom-js/zoom.js"></script>
  
  
  <script type="text/javascript" src="/lectures/reveal-js/plugin/notes/notes.js"></script>



    <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

    
  </body>
</html>
